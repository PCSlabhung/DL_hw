{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset path\n",
    "data_path_train = \"./data/training\"\n",
    "data_path_test = \"./data/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1646\n",
      "    Root location: ./data/training\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n",
      "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NFS/opt/anaconda3/envs/ml2023f/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# data transform, you can add different transform methods and resize image to any size\n",
    "img_size = 224\n",
    "transform = transforms.Compose([\n",
    "                       transforms.Resize((img_size,img_size)),\n",
    "                       transforms.ToTensor()\n",
    "                       ])\n",
    "\n",
    "#build dataset\n",
    "dataset = datasets.ImageFolder(root=data_path_train,transform=transform)\n",
    "\n",
    "# spilt your data into train and val\n",
    "TOTAL_SIZE = len(dataset)\n",
    "ratio = 0.9\n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "valid_len = round(TOTAL_SIZE * (1-ratio))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "#build dataloader\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "\n",
    "#check dataset\n",
    "print(dataset)\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(model, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in train_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    accuracy = total_correct.double() / len(train_dataset) * 100\n",
    "\n",
    "    print('Training Accuracy: {:.4f}% Training Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return \n",
    "\n",
    "#validation function\n",
    "def valid(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in val_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(val_data_loader)\n",
    "    accuracy = total_correct.double() / len(val_dataset) * 100\n",
    "\n",
    "    print('Validation Accuracy: {:.4f}% Validation Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    }
   ],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(torch.cuda.is_available()):\n",
    "    print(\"using gpu\")\n",
    "#build your model here\n",
    "\n",
    "class RN18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RN18,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64 , kernel_size = 7, stride = 2, padding = 3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding =1)\n",
    "        self.Conv1 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.Conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,kernel_size = 3, stride = 2,padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        self.shortcut1 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,kernel_size = 1, stride = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        self.Conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        self.Conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128,256,kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.shortcut2 = nn.Sequential(\n",
    "            nn.Conv2d(128,256,kernel_size = 1, stride = 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.Conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.Conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256,512,kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.shortcut3 = nn.Sequential(\n",
    "            nn.Conv2d(256,512,kernel_size = 1, stride = 2),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.Conv7 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,kernel_size = 3, stride = 1,padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size = 7, stride = 1)\n",
    "        self.FC = nn.Linear(512,1000)\n",
    "        self.Fc2 = nn.Linear(1000,4)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu1(self.Conv1(x) + x)\n",
    "        x = self.relu1(self.Conv1(x) + x)\n",
    "        x = self.relu1(self.Conv2(x) + self.shortcut1(x))\n",
    "        x = self.relu1(self.Conv3(x) + x)\n",
    "        x = self.relu1(self.Conv4(x) + self.shortcut2(x))\n",
    "        x = self.relu1(self.Conv5(x) + x)\n",
    "        x = self.relu1(self.Conv6(x) + self.shortcut3(x))\n",
    "        x = self.relu1(self.Conv7(x) + x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = self.FC(x)\n",
    "        x = self.Fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "#call model\n",
    "model = RN18()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------start training--------------\n",
      "epoch: 1\n",
      "Training Accuracy: 26.8062% Training Loss: 1.3853\n",
      "Validation Accuracy: 32.1212% Validation Loss: 1.3726\n",
      "model saved\n",
      "epoch: 2\n",
      "Training Accuracy: 36.1918% Training Loss: 1.3542\n",
      "Validation Accuracy: 35.7576% Validation Loss: 1.3522\n",
      "model saved\n",
      "epoch: 3\n",
      "Training Accuracy: 41.7286% Training Loss: 1.3376\n",
      "Validation Accuracy: 39.3939% Validation Loss: 1.3308\n",
      "model saved\n",
      "epoch: 4\n",
      "Training Accuracy: 44.7670% Training Loss: 1.3184\n",
      "Validation Accuracy: 38.7879% Validation Loss: 1.3275\n",
      "epoch: 5\n",
      "Training Accuracy: 45.0371% Training Loss: 1.3000\n",
      "Validation Accuracy: 44.2424% Validation Loss: 1.3110\n",
      "model saved\n",
      "epoch: 6\n",
      "Training Accuracy: 48.8859% Training Loss: 1.2784\n",
      "Validation Accuracy: 46.0606% Validation Loss: 1.3011\n",
      "model saved\n",
      "epoch: 7\n",
      "Training Accuracy: 49.9662% Training Loss: 1.2539\n",
      "Validation Accuracy: 49.0909% Validation Loss: 1.2243\n",
      "model saved\n",
      "epoch: 8\n",
      "Training Accuracy: 49.0209% Training Loss: 1.2297\n",
      "Validation Accuracy: 50.3030% Validation Loss: 1.2387\n",
      "model saved\n",
      "epoch: 9\n",
      "Training Accuracy: 50.8440% Training Loss: 1.2103\n",
      "Validation Accuracy: 50.9091% Validation Loss: 1.1840\n",
      "model saved\n",
      "epoch: 10\n",
      "Training Accuracy: 51.8569% Training Loss: 1.1921\n",
      "Validation Accuracy: 52.7273% Validation Loss: 1.2015\n",
      "model saved\n",
      "epoch: 11\n",
      "Training Accuracy: 54.2201% Training Loss: 1.1706\n",
      "Validation Accuracy: 53.9394% Validation Loss: 1.1542\n",
      "model saved\n",
      "epoch: 12\n",
      "Training Accuracy: 54.2876% Training Loss: 1.1474\n",
      "Validation Accuracy: 55.7576% Validation Loss: 1.1859\n",
      "model saved\n",
      "epoch: 13\n",
      "Training Accuracy: 55.7056% Training Loss: 1.1218\n",
      "Validation Accuracy: 53.9394% Validation Loss: 1.1254\n",
      "epoch: 14\n",
      "Training Accuracy: 58.0689% Training Loss: 1.0969\n",
      "Validation Accuracy: 58.7879% Validation Loss: 1.1389\n",
      "model saved\n",
      "epoch: 15\n",
      "Training Accuracy: 61.0398% Training Loss: 1.0654\n",
      "Validation Accuracy: 58.1818% Validation Loss: 1.0774\n",
      "epoch: 16\n",
      "Training Accuracy: 58.8791% Training Loss: 1.0541\n",
      "Validation Accuracy: 58.7879% Validation Loss: 1.0086\n",
      "epoch: 17\n",
      "Training Accuracy: 60.9048% Training Loss: 1.0260\n",
      "Validation Accuracy: 58.1818% Validation Loss: 1.0341\n",
      "epoch: 18\n",
      "Training Accuracy: 60.5672% Training Loss: 1.0095\n",
      "Validation Accuracy: 60.6061% Validation Loss: 1.0961\n",
      "model saved\n",
      "epoch: 19\n",
      "Training Accuracy: 63.8758% Training Loss: 0.9772\n",
      "Validation Accuracy: 62.4242% Validation Loss: 1.0711\n",
      "model saved\n",
      "epoch: 20\n",
      "Training Accuracy: 64.4159% Training Loss: 0.9538\n",
      "Validation Accuracy: 65.4545% Validation Loss: 0.9841\n",
      "model saved\n",
      "epoch: 21\n",
      "Training Accuracy: 64.1458% Training Loss: 0.9467\n",
      "Validation Accuracy: 66.0606% Validation Loss: 0.9932\n",
      "model saved\n",
      "epoch: 22\n",
      "Training Accuracy: 66.0365% Training Loss: 0.9193\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.9977\n",
      "model saved\n",
      "epoch: 23\n",
      "Training Accuracy: 65.9689% Training Loss: 0.8987\n",
      "Validation Accuracy: 64.2424% Validation Loss: 0.9938\n",
      "epoch: 24\n",
      "Training Accuracy: 66.1040% Training Loss: 0.8730\n",
      "Validation Accuracy: 62.4242% Validation Loss: 0.9949\n",
      "epoch: 25\n",
      "Training Accuracy: 68.3997% Training Loss: 0.8619\n",
      "Validation Accuracy: 59.3939% Validation Loss: 1.0180\n",
      "epoch: 26\n",
      "Training Accuracy: 67.7245% Training Loss: 0.8648\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.8984\n",
      "epoch: 27\n",
      "Training Accuracy: 69.6826% Training Loss: 0.8204\n",
      "Validation Accuracy: 68.4848% Validation Loss: 0.9074\n",
      "model saved\n",
      "epoch: 28\n",
      "Training Accuracy: 68.5348% Training Loss: 0.8253\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.9508\n",
      "epoch: 29\n",
      "Training Accuracy: 69.3450% Training Loss: 0.8136\n",
      "Validation Accuracy: 66.0606% Validation Loss: 0.8692\n",
      "epoch: 30\n",
      "Training Accuracy: 71.1006% Training Loss: 0.7877\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.8923\n",
      "epoch: 31\n",
      "Training Accuracy: 71.9109% Training Loss: 0.7842\n",
      "Validation Accuracy: 64.8485% Validation Loss: 0.9427\n",
      "epoch: 32\n",
      "Training Accuracy: 72.8562% Training Loss: 0.7716\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.8892\n",
      "epoch: 33\n",
      "Training Accuracy: 72.1134% Training Loss: 0.7538\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.9226\n",
      "epoch: 34\n",
      "Training Accuracy: 72.5186% Training Loss: 0.7376\n",
      "Validation Accuracy: 69.6970% Validation Loss: 0.8857\n",
      "model saved\n",
      "epoch: 35\n",
      "Training Accuracy: 75.2194% Training Loss: 0.7219\n",
      "Validation Accuracy: 60.6061% Validation Loss: 0.9461\n",
      "epoch: 36\n",
      "Training Accuracy: 73.8015% Training Loss: 0.7072\n",
      "Validation Accuracy: 67.2727% Validation Loss: 0.8323\n",
      "epoch: 37\n",
      "Training Accuracy: 75.7596% Training Loss: 0.6980\n",
      "Validation Accuracy: 64.2424% Validation Loss: 0.8582\n",
      "epoch: 38\n",
      "Training Accuracy: 75.9622% Training Loss: 0.6889\n",
      "Validation Accuracy: 70.3030% Validation Loss: 0.8079\n",
      "model saved\n",
      "epoch: 39\n",
      "Training Accuracy: 75.1519% Training Loss: 0.6779\n",
      "Validation Accuracy: 68.4848% Validation Loss: 0.7768\n",
      "epoch: 40\n",
      "Training Accuracy: 76.2998% Training Loss: 0.6600\n",
      "Validation Accuracy: 62.4242% Validation Loss: 0.9458\n",
      "epoch: 41\n",
      "Training Accuracy: 76.4348% Training Loss: 0.6604\n",
      "Validation Accuracy: 69.6970% Validation Loss: 0.8515\n",
      "epoch: 42\n",
      "Training Accuracy: 77.2451% Training Loss: 0.6461\n",
      "Validation Accuracy: 65.4545% Validation Loss: 0.8716\n",
      "epoch: 43\n",
      "Training Accuracy: 78.3930% Training Loss: 0.6270\n",
      "Validation Accuracy: 65.4545% Validation Loss: 0.8361\n",
      "epoch: 44\n",
      "Training Accuracy: 78.6631% Training Loss: 0.5990\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.8763\n",
      "epoch: 45\n",
      "Training Accuracy: 78.2579% Training Loss: 0.5989\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.7559\n",
      "model saved\n",
      "epoch: 46\n",
      "Training Accuracy: 79.0007% Training Loss: 0.5792\n",
      "Validation Accuracy: 69.0909% Validation Loss: 0.7684\n",
      "epoch: 47\n",
      "Training Accuracy: 80.0810% Training Loss: 0.5755\n",
      "Validation Accuracy: 57.5758% Validation Loss: 0.9309\n",
      "epoch: 48\n",
      "Training Accuracy: 80.6887% Training Loss: 0.5649\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.8235\n",
      "epoch: 49\n",
      "Training Accuracy: 81.3639% Training Loss: 0.5471\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.9022\n",
      "epoch: 50\n",
      "Training Accuracy: 81.5665% Training Loss: 0.5404\n",
      "Validation Accuracy: 67.8788% Validation Loss: 0.8017\n",
      "epoch: 51\n",
      "Training Accuracy: 82.6469% Training Loss: 0.5248\n",
      "Validation Accuracy: 54.5455% Validation Loss: 1.0059\n",
      "epoch: 52\n",
      "Training Accuracy: 82.2417% Training Loss: 0.5080\n",
      "Validation Accuracy: 72.7273% Validation Loss: 0.7804\n",
      "model saved\n",
      "epoch: 53\n",
      "Training Accuracy: 82.1067% Training Loss: 0.5086\n",
      "Validation Accuracy: 74.5455% Validation Loss: 0.7539\n",
      "model saved\n",
      "epoch: 54\n",
      "Training Accuracy: 83.2546% Training Loss: 0.4876\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.9218\n",
      "epoch: 55\n",
      "Training Accuracy: 83.1195% Training Loss: 0.4947\n",
      "Validation Accuracy: 66.6667% Validation Loss: 0.7793\n",
      "epoch: 56\n",
      "Training Accuracy: 83.3896% Training Loss: 0.4984\n",
      "Validation Accuracy: 52.1212% Validation Loss: 1.3751\n",
      "epoch: 57\n",
      "Training Accuracy: 83.9973% Training Loss: 0.4708\n",
      "Validation Accuracy: 69.0909% Validation Loss: 0.8130\n",
      "epoch: 58\n",
      "Training Accuracy: 84.6725% Training Loss: 0.4636\n",
      "Validation Accuracy: 64.8485% Validation Loss: 0.8246\n",
      "epoch: 59\n",
      "Training Accuracy: 85.3477% Training Loss: 0.4454\n",
      "Validation Accuracy: 78.1818% Validation Loss: 0.7528\n",
      "model saved\n",
      "epoch: 60\n",
      "Training Accuracy: 84.0648% Training Loss: 0.4669\n",
      "Validation Accuracy: 57.5758% Validation Loss: 0.9532\n",
      "epoch: 61\n",
      "Training Accuracy: 86.3606% Training Loss: 0.4239\n",
      "Validation Accuracy: 69.6970% Validation Loss: 0.8881\n",
      "epoch: 62\n",
      "Training Accuracy: 85.3477% Training Loss: 0.4429\n",
      "Validation Accuracy: 61.2121% Validation Loss: 0.7897\n",
      "epoch: 63\n",
      "Training Accuracy: 85.9554% Training Loss: 0.4323\n",
      "Validation Accuracy: 62.4242% Validation Loss: 0.9550\n",
      "epoch: 64\n",
      "Training Accuracy: 87.3734% Training Loss: 0.3951\n",
      "Validation Accuracy: 63.0303% Validation Loss: 0.9750\n",
      "epoch: 65\n",
      "Training Accuracy: 86.6982% Training Loss: 0.4035\n",
      "Validation Accuracy: 67.2727% Validation Loss: 0.7304\n",
      "epoch: 66\n",
      "Training Accuracy: 88.9264% Training Loss: 0.3665\n",
      "Validation Accuracy: 64.8485% Validation Loss: 0.8715\n",
      "epoch: 67\n",
      "Training Accuracy: 88.0486% Training Loss: 0.3729\n",
      "Validation Accuracy: 70.9091% Validation Loss: 0.7605\n",
      "epoch: 68\n",
      "Training Accuracy: 88.1161% Training Loss: 0.3775\n",
      "Validation Accuracy: 54.5455% Validation Loss: 1.3270\n",
      "epoch: 69\n",
      "Training Accuracy: 88.8589% Training Loss: 0.3685\n",
      "Validation Accuracy: 68.4848% Validation Loss: 0.7427\n",
      "epoch: 70\n",
      "Training Accuracy: 88.5888% Training Loss: 0.3472\n",
      "Validation Accuracy: 76.9697% Validation Loss: 0.5591\n",
      "epoch: 71\n",
      "Training Accuracy: 89.9392% Training Loss: 0.3408\n",
      "Validation Accuracy: 77.5758% Validation Loss: 0.7512\n",
      "epoch: 72\n",
      "Training Accuracy: 89.7367% Training Loss: 0.3355\n",
      "Validation Accuracy: 70.9091% Validation Loss: 0.7067\n",
      "epoch: 73\n",
      "Training Accuracy: 90.6820% Training Loss: 0.3210\n",
      "Validation Accuracy: 65.4545% Validation Loss: 0.8858\n",
      "epoch: 74\n",
      "Training Accuracy: 89.7367% Training Loss: 0.3276\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.7142\n",
      "epoch: 75\n",
      "Training Accuracy: 90.0068% Training Loss: 0.3147\n",
      "Validation Accuracy: 73.3333% Validation Loss: 0.7057\n",
      "epoch: 76\n",
      "Training Accuracy: 91.7623% Training Loss: 0.2815\n",
      "Validation Accuracy: 76.9697% Validation Loss: 0.6734\n",
      "epoch: 77\n",
      "Training Accuracy: 91.4922% Training Loss: 0.2977\n",
      "Validation Accuracy: 74.5455% Validation Loss: 0.7010\n",
      "epoch: 78\n",
      "Training Accuracy: 91.2221% Training Loss: 0.2982\n",
      "Validation Accuracy: 56.9697% Validation Loss: 1.1861\n",
      "epoch: 79\n",
      "Training Accuracy: 92.6401% Training Loss: 0.2688\n",
      "Validation Accuracy: 39.3939% Validation Loss: 2.6858\n",
      "epoch: 80\n",
      "Training Accuracy: 92.6401% Training Loss: 0.2766\n",
      "Validation Accuracy: 70.9091% Validation Loss: 0.6981\n",
      "epoch: 81\n",
      "Training Accuracy: 92.7076% Training Loss: 0.2582\n",
      "Validation Accuracy: 70.3030% Validation Loss: 0.7451\n",
      "epoch: 82\n",
      "Training Accuracy: 92.9102% Training Loss: 0.2540\n",
      "Validation Accuracy: 74.5455% Validation Loss: 0.7265\n",
      "epoch: 83\n",
      "Training Accuracy: 93.9905% Training Loss: 0.2410\n",
      "Validation Accuracy: 79.3939% Validation Loss: 0.5000\n",
      "model saved\n",
      "epoch: 84\n",
      "Training Accuracy: 92.5726% Training Loss: 0.2583\n",
      "Validation Accuracy: 75.7576% Validation Loss: 0.6555\n",
      "epoch: 85\n",
      "Training Accuracy: 94.3957% Training Loss: 0.2181\n",
      "Validation Accuracy: 76.3636% Validation Loss: 0.5257\n",
      "epoch: 86\n",
      "Training Accuracy: 93.5179% Training Loss: 0.2339\n",
      "Validation Accuracy: 77.5758% Validation Loss: 0.8194\n",
      "epoch: 87\n",
      "Training Accuracy: 94.2606% Training Loss: 0.2138\n",
      "Validation Accuracy: 72.1212% Validation Loss: 0.7851\n",
      "epoch: 88\n",
      "Training Accuracy: 94.8008% Training Loss: 0.2097\n",
      "Validation Accuracy: 71.5152% Validation Loss: 0.7810\n",
      "epoch: 89\n",
      "Training Accuracy: 95.2059% Training Loss: 0.1961\n",
      "Validation Accuracy: 72.1212% Validation Loss: 0.7810\n",
      "epoch: 90\n",
      "Training Accuracy: 94.8683% Training Loss: 0.2191\n",
      "Validation Accuracy: 70.9091% Validation Loss: 0.7036\n",
      "epoch: 91\n",
      "Training Accuracy: 95.5436% Training Loss: 0.1953\n",
      "Validation Accuracy: 80.6061% Validation Loss: 0.6053\n",
      "model saved\n",
      "epoch: 92\n",
      "Training Accuracy: 95.4760% Training Loss: 0.1862\n",
      "Validation Accuracy: 75.1515% Validation Loss: 0.9039\n",
      "epoch: 93\n",
      "Training Accuracy: 96.8940% Training Loss: 0.1802\n",
      "Validation Accuracy: 67.2727% Validation Loss: 1.0706\n",
      "epoch: 94\n",
      "Training Accuracy: 96.7589% Training Loss: 0.1718\n",
      "Validation Accuracy: 80.6061% Validation Loss: 0.5340\n",
      "epoch: 95\n",
      "Training Accuracy: 95.7461% Training Loss: 0.1734\n",
      "Validation Accuracy: 70.3030% Validation Loss: 0.9059\n",
      "epoch: 96\n",
      "Training Accuracy: 97.9068% Training Loss: 0.1435\n",
      "Validation Accuracy: 70.3030% Validation Loss: 0.8165\n",
      "epoch: 97\n",
      "Training Accuracy: 97.8393% Training Loss: 0.1435\n",
      "Validation Accuracy: 70.3030% Validation Loss: 0.9802\n",
      "epoch: 98\n",
      "Training Accuracy: 97.2991% Training Loss: 0.1464\n",
      "Validation Accuracy: 65.4545% Validation Loss: 1.1973\n",
      "epoch: 99\n",
      "Training Accuracy: 97.6367% Training Loss: 0.1385\n",
      "Validation Accuracy: 75.1515% Validation Loss: 0.7521\n",
      "epoch: 100\n",
      "Training Accuracy: 96.6914% Training Loss: 0.1460\n",
      "Validation Accuracy: 75.7576% Validation Loss: 0.5305\n"
     ]
    }
   ],
   "source": [
    "####################  implement your optimizer ###################################\n",
    "## you can use any training methods if you want (ex:lr decay, weight decay.....)\n",
    "learning_rate = 3e-4\n",
    "epochs = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# start training\n",
    "model.to(device=device)\n",
    "acc_best = 0.0\n",
    "\n",
    "print('--------------start training--------------')\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    train(model, criterion, optimizer)\n",
    "    accuracy = valid(model, criterion)\n",
    "    \n",
    "    if accuracy > acc_best:\n",
    "        acc_best = accuracy\n",
    "        print(\"model saved\")\n",
    "        # save the model\n",
    "        torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
