{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as utils\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset path\n",
    "data_path_train = \"data/training\"\n",
    "data_path_test = \"data/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1646\n",
      "    Root location: data/training\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n",
      "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NFS/opt/anaconda3/envs/ml2023f/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# data transform, you can add different transform methods and resize image to any size\n",
    "img_size = 224\n",
    "transform = transforms.Compose([\n",
    "                       transforms.Resize((img_size,img_size)),\n",
    "                       transforms.ToTensor()\n",
    "                       ])\n",
    "\n",
    "#build dataset\n",
    "dataset = datasets.ImageFolder(root=data_path_train,transform=transform)\n",
    "\n",
    "# spilt your data into train and val\n",
    "TOTAL_SIZE = len(dataset)\n",
    "ratio = 0.9\n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "valid_len = round(TOTAL_SIZE * (1-ratio))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "#build dataloader\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "\n",
    "#check dataset\n",
    "print(dataset)\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(model, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in train_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    accuracy = total_correct.double() / len(train_dataset) * 100\n",
    "\n",
    "    print('Training Accuracy: {:.4f}% Training Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return \n",
    "\n",
    "#validation function\n",
    "def valid(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for inputs, labels in val_data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "    avg_loss = total_loss / len(val_data_loader)\n",
    "    accuracy = total_correct.double() / len(val_dataset) * 100\n",
    "\n",
    "    print('Validation Accuracy: {:.4f}% Validation Loss: {:.4f}'.format(accuracy, avg_loss))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradients_in_model(model, writer, step):\n",
    "    for tag, value in model.named_parameters():\n",
    "        if value.grad is not None:\n",
    "            writer.add_histogram(tag + \"/grad\" , value.grad.cpu(), step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (1): Linear(in_features=1000, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using gpu\")\n",
    "#build your model here\n",
    "model_pretrained = torchvision.models.resnet18(pretrained = True)\n",
    "model = nn.Sequential(model_pretrained, nn.Linear(1000,4))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------start training--------------\n",
      "epoch: 1\n",
      "Training Accuracy: 84.4700% Training Loss: 0.5172\n",
      "Validation Accuracy: 82.4242% Validation Loss: 0.9121\n",
      "Epoch 1: Adam lr 0.0005000 -> 0.0004970\n",
      "model saved\n",
      "epoch: 2\n",
      "Training Accuracy: 92.5726% Training Loss: 0.2301\n",
      "Validation Accuracy: 80.0000% Validation Loss: 0.7214\n",
      "Epoch 2: Adam lr 0.0004970 -> 0.0004940\n",
      "epoch: 3\n",
      "Training Accuracy: 93.4504% Training Loss: 0.2588\n",
      "Validation Accuracy: 86.0606% Validation Loss: 0.3664\n",
      "Epoch 3: Adam lr 0.0004940 -> 0.0004910\n",
      "model saved\n",
      "epoch: 4\n",
      "Training Accuracy: 95.8136% Training Loss: 0.1301\n",
      "Validation Accuracy: 86.0606% Validation Loss: 0.5656\n",
      "Epoch 4: Adam lr 0.0004910 -> 0.0004880\n",
      "epoch: 5\n",
      "Training Accuracy: 97.1641% Training Loss: 0.0904\n",
      "Validation Accuracy: 89.0909% Validation Loss: 0.3498\n",
      "Epoch 5: Adam lr 0.0004880 -> 0.0004850\n",
      "model saved\n",
      "epoch: 6\n",
      "Training Accuracy: 97.2991% Training Loss: 0.0938\n",
      "Validation Accuracy: 87.2727% Validation Loss: 0.3364\n",
      "Epoch 6: Adam lr 0.0004850 -> 0.0004820\n",
      "epoch: 7\n",
      "Training Accuracy: 96.2863% Training Loss: 0.1553\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.2942\n",
      "Epoch 7: Adam lr 0.0004820 -> 0.0004790\n",
      "model saved\n",
      "epoch: 8\n",
      "Training Accuracy: 92.3700% Training Loss: 0.2878\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.3125\n",
      "Epoch 8: Adam lr 0.0004790 -> 0.0004760\n",
      "epoch: 9\n",
      "Training Accuracy: 93.1128% Training Loss: 0.2075\n",
      "Validation Accuracy: 89.0909% Validation Loss: 0.2618\n",
      "Epoch 9: Adam lr 0.0004760 -> 0.0004730\n",
      "epoch: 10\n",
      "Training Accuracy: 97.6367% Training Loss: 0.0754\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.5274\n",
      "Epoch 10: Adam lr 0.0004730 -> 0.0004700\n",
      "epoch: 11\n",
      "Training Accuracy: 99.1222% Training Loss: 0.0398\n",
      "Validation Accuracy: 91.5152% Validation Loss: 0.3558\n",
      "Epoch 11: Adam lr 0.0004700 -> 0.0004670\n",
      "model saved\n",
      "epoch: 12\n",
      "Training Accuracy: 98.0419% Training Loss: 0.0647\n",
      "Validation Accuracy: 91.5152% Validation Loss: 0.5402\n",
      "Epoch 12: Adam lr 0.0004670 -> 0.0004640\n",
      "epoch: 13\n",
      "Training Accuracy: 97.2316% Training Loss: 0.0934\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.2735\n",
      "Epoch 13: Adam lr 0.0004640 -> 0.0004610\n",
      "model saved\n",
      "epoch: 14\n",
      "Training Accuracy: 96.8940% Training Loss: 0.1069\n",
      "Validation Accuracy: 87.2727% Validation Loss: 0.4515\n",
      "Epoch 14: Adam lr 0.0004610 -> 0.0004580\n",
      "epoch: 15\n",
      "Training Accuracy: 97.5017% Training Loss: 0.1036\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.4119\n",
      "Epoch 15: Adam lr 0.0004580 -> 0.0004550\n",
      "epoch: 16\n",
      "Training Accuracy: 97.9068% Training Loss: 0.0800\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.4257\n",
      "Epoch 16: Adam lr 0.0004550 -> 0.0004520\n",
      "epoch: 17\n",
      "Training Accuracy: 98.0419% Training Loss: 0.0741\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.4639\n",
      "Epoch 17: Adam lr 0.0004520 -> 0.0004490\n",
      "epoch: 18\n",
      "Training Accuracy: 98.7846% Training Loss: 0.0433\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.2551\n",
      "Epoch 18: Adam lr 0.0004490 -> 0.0004460\n",
      "epoch: 19\n",
      "Training Accuracy: 98.8521% Training Loss: 0.0397\n",
      "Validation Accuracy: 88.4848% Validation Loss: 0.6502\n",
      "Epoch 19: Adam lr 0.0004460 -> 0.0004430\n",
      "epoch: 20\n",
      "Training Accuracy: 98.1094% Training Loss: 0.0569\n",
      "Validation Accuracy: 78.1818% Validation Loss: 0.8290\n",
      "Epoch 20: Adam lr 0.0004430 -> 0.0004400\n",
      "epoch: 21\n",
      "Training Accuracy: 98.1769% Training Loss: 0.0571\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.3336\n",
      "Epoch 21: Adam lr 0.0004400 -> 0.0004370\n",
      "epoch: 22\n",
      "Training Accuracy: 99.4598% Training Loss: 0.0157\n",
      "Validation Accuracy: 92.7273% Validation Loss: 0.2820\n",
      "Epoch 22: Adam lr 0.0004370 -> 0.0004340\n",
      "model saved\n",
      "epoch: 23\n",
      "Training Accuracy: 99.5273% Training Loss: 0.0130\n",
      "Validation Accuracy: 92.7273% Validation Loss: 0.2500\n",
      "Epoch 23: Adam lr 0.0004340 -> 0.0004310\n",
      "epoch: 24\n",
      "Training Accuracy: 99.4598% Training Loss: 0.0107\n",
      "Validation Accuracy: 91.5152% Validation Loss: 0.4109\n",
      "Epoch 24: Adam lr 0.0004310 -> 0.0004280\n",
      "epoch: 25\n",
      "Training Accuracy: 99.4598% Training Loss: 0.0242\n",
      "Validation Accuracy: 91.5152% Validation Loss: 0.4198\n",
      "Epoch 25: Adam lr 0.0004280 -> 0.0004250\n",
      "epoch: 26\n",
      "Training Accuracy: 99.7299% Training Loss: 0.0098\n",
      "Validation Accuracy: 91.5152% Validation Loss: 0.3894\n",
      "Epoch 26: Adam lr 0.0004250 -> 0.0004220\n",
      "epoch: 27\n",
      "Training Accuracy: 99.7299% Training Loss: 0.0142\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.4866\n",
      "Epoch 27: Adam lr 0.0004220 -> 0.0004190\n",
      "epoch: 28\n",
      "Training Accuracy: 99.0547% Training Loss: 0.0344\n",
      "Validation Accuracy: 87.8788% Validation Loss: 0.4487\n",
      "Epoch 28: Adam lr 0.0004190 -> 0.0004160\n",
      "epoch: 29\n",
      "Training Accuracy: 99.5273% Training Loss: 0.0153\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.4802\n",
      "Epoch 29: Adam lr 0.0004160 -> 0.0004130\n",
      "epoch: 30\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0043\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.4078\n",
      "Epoch 30: Adam lr 0.0004130 -> 0.0004100\n",
      "epoch: 31\n",
      "Training Accuracy: 100.0000% Training Loss: 0.0024\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.4407\n",
      "Epoch 31: Adam lr 0.0004100 -> 0.0004070\n",
      "epoch: 32\n",
      "Training Accuracy: 99.7974% Training Loss: 0.0575\n",
      "Validation Accuracy: 83.6364% Validation Loss: 0.6760\n",
      "Epoch 32: Adam lr 0.0004070 -> 0.0004040\n",
      "epoch: 33\n",
      "Training Accuracy: 97.0290% Training Loss: 0.1081\n",
      "Validation Accuracy: 78.7879% Validation Loss: 1.1623\n",
      "Epoch 33: Adam lr 0.0004040 -> 0.0004010\n",
      "epoch: 34\n",
      "Training Accuracy: 98.3120% Training Loss: 0.0612\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.5637\n",
      "Epoch 34: Adam lr 0.0004010 -> 0.0003980\n",
      "epoch: 35\n",
      "Training Accuracy: 99.5273% Training Loss: 0.0237\n",
      "Validation Accuracy: 89.0909% Validation Loss: 0.8442\n",
      "Epoch 35: Adam lr 0.0003980 -> 0.0003950\n",
      "epoch: 36\n",
      "Training Accuracy: 99.5273% Training Loss: 0.0090\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.4113\n",
      "Epoch 36: Adam lr 0.0003950 -> 0.0003920\n",
      "epoch: 37\n",
      "Training Accuracy: 99.6624% Training Loss: 0.0138\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.3960\n",
      "Epoch 37: Adam lr 0.0003920 -> 0.0003890\n",
      "epoch: 38\n",
      "Training Accuracy: 99.1222% Training Loss: 0.0212\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.3867\n",
      "Epoch 38: Adam lr 0.0003890 -> 0.0003860\n",
      "epoch: 39\n",
      "Training Accuracy: 99.4598% Training Loss: 0.0123\n",
      "Validation Accuracy: 89.6970% Validation Loss: 0.4234\n",
      "Epoch 39: Adam lr 0.0003860 -> 0.0003830\n",
      "epoch: 40\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0051\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.6400\n",
      "Epoch 40: Adam lr 0.0003830 -> 0.0003800\n",
      "epoch: 41\n",
      "Training Accuracy: 99.8650% Training Loss: 0.0059\n",
      "Validation Accuracy: 88.4848% Validation Loss: 0.7523\n",
      "Epoch 41: Adam lr 0.0003800 -> 0.0003770\n",
      "epoch: 42\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0058\n",
      "Validation Accuracy: 89.6970% Validation Loss: 0.3461\n",
      "Epoch 42: Adam lr 0.0003770 -> 0.0003740\n",
      "epoch: 43\n",
      "Training Accuracy: 99.6624% Training Loss: 0.0133\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.4030\n",
      "Epoch 43: Adam lr 0.0003740 -> 0.0003710\n",
      "epoch: 44\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0055\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.5627\n",
      "Epoch 44: Adam lr 0.0003710 -> 0.0003680\n",
      "epoch: 45\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0012\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.4621\n",
      "Epoch 45: Adam lr 0.0003680 -> 0.0003650\n",
      "epoch: 46\n",
      "Training Accuracy: 99.9325% Training Loss: 0.0463\n",
      "Validation Accuracy: 87.2727% Validation Loss: 0.5882\n",
      "Epoch 46: Adam lr 0.0003650 -> 0.0003620\n",
      "epoch: 47\n",
      "Training Accuracy: 94.1931% Training Loss: 0.1929\n",
      "Validation Accuracy: 84.8485% Validation Loss: 0.5479\n",
      "Epoch 47: Adam lr 0.0003620 -> 0.0003590\n",
      "epoch: 48\n",
      "Training Accuracy: 98.9196% Training Loss: 0.0493\n",
      "Validation Accuracy: 88.4848% Validation Loss: 0.4802\n",
      "Epoch 48: Adam lr 0.0003590 -> 0.0003560\n",
      "epoch: 49\n",
      "Training Accuracy: 99.5949% Training Loss: 0.0183\n",
      "Validation Accuracy: 90.9091% Validation Loss: 0.3027\n",
      "Epoch 49: Adam lr 0.0003560 -> 0.0003530\n",
      "epoch: 50\n",
      "Training Accuracy: 98.2444% Training Loss: 0.0490\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.4897\n",
      "Epoch 50: Adam lr 0.0003530 -> 0.0003500\n",
      "epoch: 51\n",
      "Training Accuracy: 99.0547% Training Loss: 0.0334\n",
      "Validation Accuracy: 89.0909% Validation Loss: 0.3926\n",
      "Epoch 51: Adam lr 0.0003500 -> 0.0003470\n",
      "epoch: 52\n",
      "Training Accuracy: 99.3923% Training Loss: 0.0156\n",
      "Validation Accuracy: 90.3030% Validation Loss: 0.6446\n",
      "Epoch 52: Adam lr 0.0003470 -> 0.0003440\n",
      "epoch: 53\n",
      "Training Accuracy: 99.4598% Training Loss: 0.0401\n",
      "Validation Accuracy: 89.6970% Validation Loss: 0.4742\n",
      "Epoch 53: Adam lr 0.0003440 -> 0.0003410\n",
      "epoch: 54\n",
      "Training Accuracy: 98.7171% Training Loss: 0.0672\n",
      "Validation Accuracy: 92.1212% Validation Loss: 0.4527\n",
      "Epoch 54: Adam lr 0.0003410 -> 0.0003380\n",
      "epoch: 55\n",
      "Training Accuracy: 97.9068% Training Loss: 0.0672\n",
      "Validation Accuracy: 89.0909% Validation Loss: 0.4291\n",
      "Epoch 55: Adam lr 0.0003380 -> 0.0003350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_histogram(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights\u001b[39m\u001b[38;5;124m'\u001b[39m, model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mweight, epoch)\n\u001b[1;32m     24\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, after_lr, epoch)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mlog_gradients_in_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m acc_best:\n\u001b[1;32m     27\u001b[0m     acc_best \u001b[38;5;241m=\u001b[39m accuracy\n",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mlog_gradients_in_model\u001b[0;34m(model, writer, step)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_gradients_in_model\u001b[39m(model, writer, step):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag, value \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m             writer\u001b[38;5;241m.\u001b[39madd_histogram(tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/grad\u001b[39m\u001b[38;5;124m\"\u001b[39m , value\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcpu(), step)\n",
      "File \u001b[0;32m/home/NFS/opt/anaconda3/envs/ml2023f/lib/python3.8/site-packages/torch/nn/modules/module.py:2115\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   2114\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate)\n\u001b[0;32m-> 2115\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m/home/NFS/opt/anaconda3/envs/ml2023f/lib/python3.8/site-packages/torch/nn/modules/module.py:2052\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2050\u001b[0m members \u001b[38;5;241m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################  implement your optimizer ###################################\n",
    "## you can use any training methods if you want (ex:lr decay, weight decay.....)\n",
    "learning_rate = 5e-4\n",
    "epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lambda1 = lambda epoch : 0.2 * epoch if epoch > 5 else 1\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.4, total_iters=100)\n",
    "# start training\n",
    "model.to(device=device)\n",
    "acc_best = 0.0\n",
    "\n",
    "print('--------------start training--------------')\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    train(model, criterion, optimizer)\n",
    "    accuracy = valid(model, criterion)\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: Adam lr %.7f -> %.7f\" % (epoch, before_lr, after_lr))\n",
    "    writer.add_histogram('Weights', model[0].conv1.weight, epoch)\n",
    "    writer.add_scalar('Learning_rate', after_lr, epoch)\n",
    "    log_gradients_in_model(model, writer, epoch)\n",
    "    if accuracy > acc_best:\n",
    "        acc_best = accuracy\n",
    "        print(\"model saved\")\n",
    "        torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "dataset_test = datasets.ImageFolder(root=data_path_test, transform=transform_test)\n",
    "dataloader_test  = data.DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model so that you don't need to train the model again\n",
    "test_model = torch.load(\"model.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        bs = dataloader_test.batch_size\n",
    "        result = []\n",
    "        for i, (data, target) in enumerate(dataloader_test):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1, keepdim=True)\n",
    "            \n",
    "            arr = preds.data.cpu().numpy()\n",
    "            for j in range(preds.size()[0]):\n",
    "                file_name = dataset_test.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,preds[j].cpu().numpy()[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('ID_result.csv','w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
