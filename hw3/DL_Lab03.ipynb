{"cells":[{"cell_type":"markdown","metadata":{"id":"C_jdZ5vHJ4A9"},"source":["# Task description\n","- Identity the speakers of given features.\n","- Main goal: Get familiar with transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhLNWB-AK2Z5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ecafa5c-816d-4bec-84f9-646d1dffc39d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n","Dataset/\n","Dataset/uttr-bf3c7ba5bd8e4b8fb22737fda57fa37f.pt\n"]}],"source":["# unzip the file\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/Colab Notebooks\"\n","!tar zxvf Dataset.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"ENWVAUDVJtVY"},"source":["## Fix Random Seed"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"E6burzCXIyuA","executionInfo":{"status":"ok","timestamp":1700553124242,"user_tz":-480,"elapsed":3893,"user":{"displayName":"あおきじfujitola","userId":"08461597634094197022"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_seed(87)"]},{"cell_type":"markdown","metadata":{"id":"k7dVbxW2LASN"},"source":["# Data\n","\n","## Dataset\n","- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n","- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n","- We randomly select 600 speakers from Voxceleb2.\n","- Then preprocess the raw waveforms into mel-spectrograms.\n","\n","- Args:\n","  - data_dir: The path to the data directory.\n","  - metadata_path: The path to the metadata.\n","  - segment_len: The length of audio segment for training.\n","- The architecture of data directory \\\\\n","  - data directory \\\\\n","  |---- metadata.json \\\\\n","  |---- testdata.json \\\\\n","  |---- mapping.json \\\\\n","  |---- uttr-{random string}.pt \\\\\n","\n","- The information in metadata\n","  - \"n_mels\": The dimention of mel-spectrogram.\n","  - \"speakers\": A dictionary.\n","    - Key: speaker ids.\n","    - value: \"feature_path\" and \"mel_len\"\n","\n","\n","For efficiency, we segment the mel-spectrograms into segments in the traing step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpuGxl4CI2pr"},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","import torch.nn as nn\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","class myDataset(Dataset):\n","\tdef __init__(self, data_dir, segment_len=128):\n","\t\tself.data_dir = data_dir\n","\t\tself.segment_len = segment_len\n","\n","\t\t# Load the mapping from speaker neme to their corresponding id.\n","\t\tmapping_path = Path(data_dir) / \"mapping.json\"\n","\t\tmapping = json.load(mapping_path.open())\n","\t\tself.speaker2id = mapping[\"speaker2id\"]\n","\n","\t\t# Load metadata of training data.\n","\t\tmetadata_path = Path(data_dir) / \"metadata.json\"\n","\t\tmetadata = json.load(open(metadata_path))[\"speakers\"]\n","\n","\t\t# Get the total number of speaker.\n","\t\tself.speaker_num = len(metadata.keys())\n","\t\tself.data = []\n","\t\tfor speaker in metadata.keys():\n","\t\t\tfor utterances in metadata[speaker]:\n","\t\t\t\tself.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n","\n","\tdef __len__(self):\n","\t\t\treturn len(self.data)\n","\n","\tdef __getitem__(self, index):\n","\t\tfeat_path, speaker = self.data[index]\n","\t\t# Load preprocessed mel-spectrogram.\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","\t\t# Segmemt mel-spectrogram into \"segment_len\" frames.\n","\t\tif len(mel) > self.segment_len:\n","\t\t\t# Randomly get the starting point of the segment.\n","\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n","\t\t\t# Get a segment with \"segment_len\" frames.\n","\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n","\t\telse:\n","\t\t\tmel = torch.FloatTensor(mel)\n","\t\t# Turn the speaker id into long for computing loss later.\n","\t\tspeaker = torch.FloatTensor([speaker]).long()\n","\t\treturn mel, speaker\n","\n","\tdef get_speaker_number(self):\n","\t\treturn self.speaker_num"]},{"cell_type":"markdown","metadata":{"id":"668hverTMlGN"},"source":["## Dataloader\n","- Split dataset into training dataset(90%) and validation dataset(10%).\n","- Create dataloader to iterate the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7c2gZYoJDRS"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","def collate_batch(batch):\n","\t# Process features within a batch.\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tmel, speaker = zip(*batch)\n","\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n","\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n","\t# mel: (batch size, length, 40)\n","\treturn mel, torch.FloatTensor(speaker).long()\n","\n","\n","def get_dataloader(data_dir, batch_size, n_workers):\n","\t\"\"\"Generate dataloader\"\"\"\n","\tdataset = myDataset(data_dir)\n","\tspeaker_num = dataset.get_speaker_number()\n","\t# Split dataset into training dataset and validation dataset\n","\ttrainlen = int(0.9 * len(dataset))\n","\tlengths = [trainlen, len(dataset) - trainlen]\n","\ttrainset, validset = random_split(dataset, lengths)\n","\n","\ttrain_loader = DataLoader(\n","\t\ttrainset,\n","\t\tbatch_size=batch_size,\n","\t\tshuffle=True,\n","\t\tdrop_last=True,\n","\t\tnum_workers=n_workers,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\tvalid_loader = DataLoader(\n","\t\tvalidset,\n","\t\tbatch_size=batch_size,\n","\t\tnum_workers=n_workers,\n","\t\tdrop_last=True,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\n","\treturn train_loader, valid_loader, speaker_num"]},{"cell_type":"markdown","metadata":{"id":"5FOSZYxrMqhc"},"source":["# Model\n","- Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","    - TransformerEncoderLayer:\n","    - TransformerEncoder:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUHNLn-yeWPB"},"outputs":[],"source":["\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_head):\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.n_head = n_head\n","        self.dim_head = d_model\n","        self.W_Q = nn.Linear(self.dim_head, self.dim_head * self.n_head, bias = False)\n","        self.W_K = nn.Linear(self.dim_head, self.dim_head * self.n_head, bias = False)\n","        self.W_V = nn.Linear(self.dim_head, self.dim_head * self.n_head, bias = False)\n","        self.fc = nn.Linear(self.n_head *self.dim_head, d_model )\n","    def forward(self, values, query, keys):\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.n_head, self.dim_head)\n","        keys = keys.reshape(N, key_len, self.n_head, self.dim_head)\n","        queries = query.reshape(N, query_len, self.n_head, self.dim_head)\n","\n","        values = self.W_V(values)\n","        keys = self.W_K(keys)\n","        queries = self.W_Q(queries)\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","\n","        attention = torch.nn.functional.softmax(energy / (self.d_model ** (1 / 2)), dim=3)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.n_head * self.dim_head\n","        )\n","\n","        out = self.fc(out)\n","        return out\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, dim_feedforward, nhead, dropout = 0.1):\n","        super().__init__()\n","        self.enc_self_attn = MultiHeadAttention(d_model , nhead)\n","        self.Norm1 = nn.LayerNorm(d_model)\n","        self.Norm2 = nn.LayerNorm(d_model)\n","        self.feedforward = nn.Sequential(\n","            nn.Linear(d_model, dim_feedforward * d_model),\n","            nn.ReLU(),\n","            nn.Linear(dim_feedforward * d_model, d_model)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        attention = self.enc_self_attn(x,x,x)\n","        x = self.dropout(self.Norm1(attention + x))\n","        forward = self.feedforward(x)\n","        out = self.dropout(forward)\n","        return out\n","class TransformerEncoder(nn.Module):\n","    def __init__(self,d_model, encode_layer, num_layers):\n","        super().__init__()\n","        self.layers = nn.ModuleList(\n","            [\n","                encode_layer\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","    def forward(self, x):\n","        N = x.shape\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXZ5B0EKJGs8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700497543724,"user_tz":-480,"elapsed":283,"user":{"displayName":"あおきじfujitola","userId":"08461597634094197022"}},"outputId":"08d06094-7649-4334-9b1e-f3af6298306d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The parameter size of encoder block is 154.88k\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Classifier(nn.Module):\n","\tdef __init__(self, d_model= 80, n_spks=600, dropout=0.1):\n","\t\tsuper().__init__()\n","\t\t# Project the dimension of features from that of input into d_model.\n","\t\tself.prenet = nn.Linear(40, d_model)\n","\t\t# TODO:\n","\t\t#   Build vanilla transformer encoder block from scratch !!!\n","        #   You can't call the transformer encoder block function from PyTorch library !!!\n","        #   The number of encoder layers should be less than 3 !!!\n","        #   The parameter size of transformer encoder block should be less than 500k !!!\n","\t\tself.encoder_layer = TransformerEncoderLayer(d_model, dim_feedforward=10,nhead=1) #your own transformer encoder layer\n","\t\tself.encoder = TransformerEncoder(d_model, self.encoder_layer,num_layers=3)#your own transformer encoder\n","\t\t# Project the the dimension of features from d_model into speaker nums.\n","\t\tself.pred_layer = nn.Sequential(\n","\t\t\tnn.Linear(d_model, d_model),\n","\t\t\tnn.ReLU(),\n","\t\t\tnn.Linear(d_model, n_spks),\n","\t\t)\n","\n","\tdef forward(self, mels):\n","\t\tout = self.prenet(mels)\n","\t\tout = out.permute(1, 0, 2)\n","\t\t# The encoder layer expect features in the shape of (length, batch size, d_model).\n","\t\tout = self.encoder_layer(out)\n","\t\tout = out.transpose(0, 1)\n","\t\t# mean pooling\n","\t\tstats = out.mean(dim=1)\n","\t\tout = self.pred_layer(stats)\n","\t\treturn out\n","\n","encoderblock = Classifier()\n","param_enc = sum(p.numel() for p in encoderblock.encoder.parameters())\n","print (f\"The parameter size of encoder block is {param_enc/1000}k\")"]},{"cell_type":"markdown","metadata":{"id":"W7yX8JinM5Ly"},"source":["# Learning rate schedule\n","- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n","- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n","- The warmup schedule\n","  - Set learning rate to 0 in the beginning.\n","  - The learning rate increases linearly from 0 to initial learning rate during warmup period."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykt0N1nVJJi2"},"outputs":[],"source":["import math\n","\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","\n","def get_cosine_schedule_with_warmup(\n","    optimizer: Optimizer,\n","    num_warmup_steps: int,\n","    num_training_steps: int,\n","    num_cycles: float = 0.5,\n","    last_epoch: int = -1,\n","):\n","\tdef lr_lambda(current_step):\n","\t\t# Warmup\n","\t\tif current_step < num_warmup_steps:\n","\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n","\t\t# decadence\n","\t\tprogress = float(current_step - num_warmup_steps) / float(\n","\t\t\tmax(1, num_training_steps - num_warmup_steps)\n","\t\t)\n","\t\treturn max(\n","\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n","\t\t)\n","\n","\treturn LambdaLR(optimizer, lr_lambda, last_epoch)"]},{"cell_type":"markdown","metadata":{"id":"-LN2XkteM_uH"},"source":["# Model Function\n","- Model forward function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-rr8529JMz0"},"outputs":[],"source":["import torch\n","\n","\n","def model_fn(batch, model, criterion, device):\n","\t\"\"\"Forward a batch through the model.\"\"\"\n","\n","\tmels, labels = batch\n","\tmels = mels.to(device)\n","\tlabels = labels.to(device)\n","\n","\touts = model(mels)\n","\n","\tloss = criterion(outs, labels)\n","\n","\t# Get the speaker id with highest probability.\n","\tpreds = outs.argmax(1)\n","\t# Compute accuracy.\n","\taccuracy = torch.mean((preds == labels).float())\n","\n","\treturn loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"cwM_xyOtNCI2"},"source":["# Validate\n","- Calculate accuracy of the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAiv6kpdJRTJ"},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","\n","\n","def valid(dataloader, model, criterion, device):\n","\t\"\"\"Validate on validation set.\"\"\"\n","\n","\tmodel.eval()\n","\trunning_loss = 0.0\n","\trunning_accuracy = 0.0\n","\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n","\n","\tfor i, batch in enumerate(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\t\trunning_loss += loss.item()\n","\t\t\trunning_accuracy += accuracy.item()\n","\n","\t\tpbar.update(dataloader.batch_size)\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n","\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n","\t\t)\n","\n","\tpbar.close()\n","\tmodel.train()\n","\n","\treturn running_accuracy / len(dataloader)"]},{"cell_type":"markdown","metadata":{"id":"g6ne9G-eNEdG"},"source":["# Main function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Usv9s-CuJSG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700503930474,"user_tz":-480,"elapsed":6375949,"user":{"displayName":"あおきじfujitola","userId":"08461597634094197022"}},"outputId":"43106a05-01fb-4464-f808-86dcf1cb2189"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n"]},{"output_type":"stream","name":"stderr","text":["Train:   0% 0/2000 [06:32<?, ? step/s]\n","Train:   0% 0/2000 [05:13<?, ? step/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.12, loss=4.08, step=2000]\n","Valid: 100% 5664/5667 [00:16<00:00, 345.81 uttr/s, accuracy=0.13, loss=4.28]\n","Train: 100% 2000/2000 [02:41<00:00, 12.39 step/s, accuracy=0.22, loss=3.65, step=4000]\n","Valid: 100% 5664/5667 [00:16<00:00, 343.01 uttr/s, accuracy=0.24, loss=3.59]\n","Train: 100% 2000/2000 [02:41<00:00, 12.40 step/s, accuracy=0.47, loss=2.40, step=6000]\n","Valid: 100% 5664/5667 [00:16<00:00, 349.93 uttr/s, accuracy=0.30, loss=3.21]\n","Train: 100% 2000/2000 [02:42<00:00, 12.28 step/s, accuracy=0.28, loss=2.95, step=8000]\n","Valid: 100% 5664/5667 [00:15<00:00, 360.24 uttr/s, accuracy=0.36, loss=2.96]\n","Train: 100% 2000/2000 [02:39<00:00, 12.54 step/s, accuracy=0.47, loss=2.12, step=1e+4]\n","Valid: 100% 5664/5667 [00:17<00:00, 329.53 uttr/s, accuracy=0.39, loss=2.80]\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","\n","\n","Train:   0% 10/2000 [00:00<00:57, 34.36 step/s, accuracy=0.38, loss=3.13, step=1e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 10000, best model saved. (accuracy=0.3893)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:42<00:00, 12.30 step/s, accuracy=0.38, loss=2.97, step=12000]\n","Valid: 100% 5664/5667 [00:16<00:00, 343.08 uttr/s, accuracy=0.39, loss=2.75]\n","Train: 100% 2000/2000 [02:43<00:00, 12.24 step/s, accuracy=0.31, loss=2.78, step=14000]\n","Valid: 100% 5664/5667 [00:21<00:00, 266.88 uttr/s, accuracy=0.43, loss=2.60]\n","Train: 100% 2000/2000 [02:48<00:00, 11.90 step/s, accuracy=0.25, loss=2.44, step=16000]\n","Valid: 100% 5664/5667 [00:16<00:00, 339.78 uttr/s, accuracy=0.44, loss=2.50]\n","Train: 100% 2000/2000 [02:40<00:00, 12.49 step/s, accuracy=0.34, loss=2.14, step=18000]\n","Valid: 100% 5664/5667 [00:17<00:00, 331.41 uttr/s, accuracy=0.49, loss=2.35]\n","Train: 100% 2000/2000 [02:42<00:00, 12.31 step/s, accuracy=0.38, loss=2.34, step=2e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 345.36 uttr/s, accuracy=0.48, loss=2.34]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   1% 11/2000 [00:00<00:48, 40.74 step/s, accuracy=0.69, loss=1.66, step=2e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 20000, best model saved. (accuracy=0.4857)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:44<00:00, 12.12 step/s, accuracy=0.56, loss=1.51, step=22000]\n","Valid: 100% 5664/5667 [00:16<00:00, 338.73 uttr/s, accuracy=0.52, loss=2.15]\n","Train: 100% 2000/2000 [02:43<00:00, 12.24 step/s, accuracy=0.47, loss=2.39, step=24000]\n","Valid: 100% 5664/5667 [00:15<00:00, 356.71 uttr/s, accuracy=0.52, loss=2.17]\n","Train: 100% 2000/2000 [02:40<00:00, 12.43 step/s, accuracy=0.59, loss=1.83, step=26000]\n","Valid: 100% 5664/5667 [00:16<00:00, 333.39 uttr/s, accuracy=0.54, loss=2.07]\n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.69, loss=1.65, step=28000]\n","Valid: 100% 5664/5667 [00:16<00:00, 337.13 uttr/s, accuracy=0.55, loss=2.03]\n","Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=0.47, loss=2.31, step=3e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 346.71 uttr/s, accuracy=0.54, loss=2.02]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   0% 5/2000 [00:00<01:53, 17.56 step/s, accuracy=0.50, loss=2.35, step=3e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 30000, best model saved. (accuracy=0.5475)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:41<00:00, 12.37 step/s, accuracy=0.59, loss=1.57, step=32000]\n","Valid: 100% 5664/5667 [00:15<00:00, 356.66 uttr/s, accuracy=0.56, loss=1.94]\n","Train: 100% 2000/2000 [02:39<00:00, 12.52 step/s, accuracy=0.69, loss=1.35, step=34000]\n","Valid: 100% 5664/5667 [00:15<00:00, 356.22 uttr/s, accuracy=0.56, loss=1.94]\n","Train: 100% 2000/2000 [02:40<00:00, 12.47 step/s, accuracy=0.56, loss=1.61, step=36000]\n","Valid: 100% 5664/5667 [00:16<00:00, 347.02 uttr/s, accuracy=0.58, loss=1.87]\n","Train: 100% 2000/2000 [02:44<00:00, 12.18 step/s, accuracy=0.44, loss=1.95, step=38000]\n","Valid: 100% 5664/5667 [00:16<00:00, 340.61 uttr/s, accuracy=0.59, loss=1.80]\n","Train: 100% 2000/2000 [02:49<00:00, 11.82 step/s, accuracy=0.66, loss=1.04, step=4e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 336.13 uttr/s, accuracy=0.59, loss=1.79]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   1% 11/2000 [00:00<00:49, 40.44 step/s, accuracy=0.41, loss=2.45, step=4e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 40000, best model saved. (accuracy=0.5950)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:41<00:00, 12.38 step/s, accuracy=0.62, loss=1.46, step=42000]\n","Valid: 100% 5664/5667 [00:16<00:00, 348.55 uttr/s, accuracy=0.60, loss=1.78]\n","Train: 100% 2000/2000 [02:44<00:00, 12.16 step/s, accuracy=0.78, loss=1.14, step=44000]\n","Valid: 100% 5664/5667 [00:16<00:00, 348.31 uttr/s, accuracy=0.61, loss=1.76]\n","Train: 100% 2000/2000 [02:44<00:00, 12.18 step/s, accuracy=0.72, loss=0.92, step=46000]\n","Valid: 100% 5664/5667 [00:16<00:00, 344.45 uttr/s, accuracy=0.62, loss=1.71]\n","Train: 100% 2000/2000 [02:40<00:00, 12.42 step/s, accuracy=0.66, loss=1.17, step=48000]\n","Valid: 100% 5664/5667 [00:16<00:00, 350.91 uttr/s, accuracy=0.62, loss=1.68]\n","Train: 100% 2000/2000 [02:41<00:00, 12.36 step/s, accuracy=0.62, loss=1.17, step=5e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 347.81 uttr/s, accuracy=0.63, loss=1.67]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   1% 13/2000 [00:00<00:36, 53.86 step/s, accuracy=0.78, loss=1.20, step=5e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 50000, best model saved. (accuracy=0.6299)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:42<00:00, 12.31 step/s, accuracy=0.66, loss=1.50, step=52000]\n","Valid: 100% 5664/5667 [00:16<00:00, 346.46 uttr/s, accuracy=0.64, loss=1.62]\n","Train: 100% 2000/2000 [02:44<00:00, 12.13 step/s, accuracy=0.50, loss=2.35, step=54000]\n","Valid: 100% 5664/5667 [00:16<00:00, 344.26 uttr/s, accuracy=0.64, loss=1.64]\n","Train: 100% 2000/2000 [02:47<00:00, 11.93 step/s, accuracy=0.66, loss=1.75, step=56000]\n","Valid: 100% 5664/5667 [00:16<00:00, 341.92 uttr/s, accuracy=0.63, loss=1.65]\n","Train: 100% 2000/2000 [02:42<00:00, 12.33 step/s, accuracy=0.72, loss=1.40, step=58000]\n","Valid: 100% 5664/5667 [00:16<00:00, 344.22 uttr/s, accuracy=0.66, loss=1.58]\n","Train: 100% 2000/2000 [02:44<00:00, 12.19 step/s, accuracy=0.56, loss=1.77, step=6e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 335.52 uttr/s, accuracy=0.65, loss=1.58]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   0% 7/2000 [00:00<01:10, 28.29 step/s, accuracy=0.72, loss=0.83, step=6e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 60000, best model saved. (accuracy=0.6555)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.78, loss=1.04, step=62000]\n","Valid: 100% 5664/5667 [00:16<00:00, 344.24 uttr/s, accuracy=0.66, loss=1.55]\n","Train: 100% 2000/2000 [02:44<00:00, 12.16 step/s, accuracy=0.69, loss=1.04, step=64000]\n","Valid: 100% 5664/5667 [00:16<00:00, 338.64 uttr/s, accuracy=0.65, loss=1.59]\n","Train: 100% 2000/2000 [02:43<00:00, 12.20 step/s, accuracy=0.69, loss=1.04, step=66000]\n","Valid: 100% 5664/5667 [00:16<00:00, 340.11 uttr/s, accuracy=0.66, loss=1.56]\n","Train: 100% 2000/2000 [02:44<00:00, 12.19 step/s, accuracy=0.78, loss=0.87, step=68000]\n","Valid: 100% 5664/5667 [00:16<00:00, 335.40 uttr/s, accuracy=0.66, loss=1.54]\n","Train: 100% 2000/2000 [02:44<00:00, 12.16 step/s, accuracy=0.72, loss=1.48, step=7e+4]\n","Valid: 100% 5664/5667 [00:16<00:00, 333.63 uttr/s, accuracy=0.66, loss=1.55]\n","\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n","\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n"]},{"output_type":"stream","name":"stdout","text":["Step 70000, best model saved. (accuracy=0.6614)\n"]}],"source":["from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"./Dataset\",\n","\t\t\"save_path\": \"model.ckpt\",\n","\t\t\"batch_size\": 32,\n","\t\t\"n_workers\": 8,\n","\t\t\"valid_steps\": 2000,\n","\t\t\"warmup_steps\": 1000,\n","\t\t\"save_steps\": 10000,\n","\t\t\"total_steps\": 70000,\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tsave_path,\n","\tbatch_size,\n","\tn_workers,\n","\tvalid_steps,\n","\twarmup_steps,\n","\ttotal_steps,\n","\tsave_steps,\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\n","\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n","\ttrain_iterator = iter(train_loader)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\tmodel = Classifier(n_spks=speaker_num).to(device)\n","\tcriterion = nn.CrossEntropyLoss()\n","\toptimizer = AdamW(model.parameters(), lr=1e-3)\n","\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\tbest_accuracy = -1.0\n","\tbest_state_dict = None\n","\n","\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\tfor step in range(total_steps):\n","\t\t# Get data\n","\t\ttry:\n","\t\t\tbatch = next(train_iterator)\n","\t\texcept StopIteration:\n","\t\t\ttrain_iterator = iter(train_loader)\n","\t\t\tbatch = next(train_iterator)\n","\n","\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\tbatch_loss = loss.item()\n","\t\tbatch_accuracy = accuracy.item()\n","\n","\t\t# Updata model\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\tscheduler.step()\n","\t\toptimizer.zero_grad()\n","\n","\t\t# Log\n","\t\tpbar.update()\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{batch_loss:.2f}\",\n","\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n","\t\t\tstep=step + 1,\n","\t\t)\n","\n","\t\t# Do validation\n","\t\tif (step + 1) % valid_steps == 0:\n","\t\t\tpbar.close()\n","\n","\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n","\n","\t\t\t# keep the best model\n","\t\t\tif valid_accuracy > best_accuracy:\n","\t\t\t\tbest_accuracy = valid_accuracy\n","\t\t\t\tbest_state_dict = model.state_dict()\n","\n","\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\t\t# Save the best model so far.\n","\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n","\t\t\ttorch.save(best_state_dict, save_path)\n","\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n","\n","\tpbar.close()\n","\n","\n","if __name__ == \"__main__\":\n","\tmain(**parse_args())"]},{"cell_type":"markdown","metadata":{"id":"NLatBYAhNNMx"},"source":["# Inference\n","\n","## Dataset of inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efS4pCmAJXJH"},"outputs":[],"source":["import os\n","import json\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","class InferenceDataset(Dataset):\n","\tdef __init__(self, data_dir):\n","\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n","\t\tmetadata = json.load(testdata_path.open())\n","\t\tself.data_dir = data_dir\n","\t\tself.data = metadata[\"utterances\"]\n","\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\n","\tdef __getitem__(self, index):\n","\t\tutterance = self.data[index]\n","\t\tfeat_path = utterance[\"feature_path\"]\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","\t\treturn feat_path, mel\n","\n","\n","def inference_collate_batch(batch):\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tfeat_paths, mels = zip(*batch)\n","\n","\treturn feat_paths, torch.stack(mels)"]},{"cell_type":"markdown","metadata":{"id":"tl0WnYwxNK_S"},"source":["## Main funcrion of Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["4c7c8070691445f082d828de6b3021b8","9885809e4ab145d7842385fdbe5da5f1","b0798d9a1e01459bad35d24a40cff881","fbc02839fdc442cdb97ce13855c991f3","6901180fb4504e179d097cd02b22a159","73d42bd20f6b4176863388bb361fc5d6","1fec08aeb9de4eeda3b2060d836a0ca4","de26ccddfe6f4da09b46805e86832d8a","e0c8811f34ff4ae9a3fd4e4ba3efedcc","b9b04df160474700bd74bd0bc5e9d2ad","2774f246814c4dc9b2da7168ee9c1b84"]},"id":"i8SAbuXEJb2A","executionInfo":{"status":"ok","timestamp":1700504194747,"user_tz":-480,"elapsed":36332,"user":{"displayName":"あおきじfujitola","userId":"08461597634094197022"}},"outputId":"78c75d8e-0bab-4548-ff54-36b1beedb1ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7c8070691445f082d828de6b3021b8"}},"metadata":{}}],"source":["import json\n","import csv\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torchsummary import summary\n","from torch.utils.data import DataLoader\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"./Dataset\",\n","\t\t\"model_path\": \"./model.ckpt\",\n","\t\t\"output_path\": \"./output.csv\",\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tmodel_path,\n","\toutput_path,\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\n","\tmapping_path = Path(data_dir) / \"mapping.json\"\n","\tmapping = json.load(mapping_path.open())\n","\n","\tdataset = InferenceDataset(data_dir)\n","\tdataloader = DataLoader(\n","\t\tdataset,\n","\t\tbatch_size=1,\n","\t\tshuffle=False,\n","\t\tdrop_last=False,\n","\t\tnum_workers=8,\n","\t\tcollate_fn=inference_collate_batch,\n","\t)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\tspeaker_num = len(mapping[\"id2speaker\"])\n","\tmodel = Classifier(n_spks=speaker_num).to(device)\n","\tmodel.load_state_dict(torch.load(model_path))\n","\tmodel.eval()\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\tresults = [[\"Id\", \"Category\"]]\n","\tfor feat_paths, mels in tqdm(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tmels = mels.to(device)\n","\t\t\touts = model(mels)\n","\t\t\tpreds = outs.argmax(1).cpu().numpy()\n","\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n","\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n","\n","\twith open(output_path, 'w', newline='') as csvfile:\n","\t\twriter = csv.writer(csvfile)\n","\t\twriter.writerows(results)\n","\n","\n","if __name__ == \"__main__\":\n","\tmain(**parse_args())"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4c7c8070691445f082d828de6b3021b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9885809e4ab145d7842385fdbe5da5f1","IPY_MODEL_b0798d9a1e01459bad35d24a40cff881","IPY_MODEL_fbc02839fdc442cdb97ce13855c991f3"],"layout":"IPY_MODEL_6901180fb4504e179d097cd02b22a159"}},"9885809e4ab145d7842385fdbe5da5f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73d42bd20f6b4176863388bb361fc5d6","placeholder":"​","style":"IPY_MODEL_1fec08aeb9de4eeda3b2060d836a0ca4","value":"100%"}},"b0798d9a1e01459bad35d24a40cff881":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de26ccddfe6f4da09b46805e86832d8a","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0c8811f34ff4ae9a3fd4e4ba3efedcc","value":8000}},"fbc02839fdc442cdb97ce13855c991f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9b04df160474700bd74bd0bc5e9d2ad","placeholder":"​","style":"IPY_MODEL_2774f246814c4dc9b2da7168ee9c1b84","value":" 8000/8000 [00:36&lt;00:00, 256.35it/s]"}},"6901180fb4504e179d097cd02b22a159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d42bd20f6b4176863388bb361fc5d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fec08aeb9de4eeda3b2060d836a0ca4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de26ccddfe6f4da09b46805e86832d8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c8811f34ff4ae9a3fd4e4ba3efedcc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9b04df160474700bd74bd0bc5e9d2ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2774f246814c4dc9b2da7168ee9c1b84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}